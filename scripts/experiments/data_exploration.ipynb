{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efbdafb1",
   "metadata": {},
   "source": [
    "### Model Stats CSV\n",
    "Show info about the CSV file<br>\n",
    "Show percentage of \"is_better\" overall and for each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7876600c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4320, 550)\n",
      "\n",
      "Percentage of 'is_better' for each dataset:\n",
      "dataset_name  dataset_group  scores_is_better\n",
      "Gluonts       m1_monthly     True                70.138889\n",
      "                             False               29.861111\n",
      "              m1_quarterly   True                93.055556\n",
      "                             False                6.944444\n",
      "M3            Monthly        True                82.638889\n",
      "                             False               17.361111\n",
      "              Quarterly      True                59.027778\n",
      "                             False               40.972222\n",
      "Tourism       Monthly        False               90.277778\n",
      "                             True                 9.722222\n",
      "              Quarterly      False               81.944444\n",
      "                             True                18.055556\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Percentage of 'is_better' overall:\n",
      "scores_is_better\n",
      "True     55.439815\n",
      "False    44.560185\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./model_stats.csv')\n",
    "print(df.shape)\n",
    "\n",
    "# Print percentage of \"is_better\" overall and for each dataset\n",
    "print(\"\\nPercentage of 'is_better' for each dataset:\")\n",
    "print((df.groupby(['dataset_name', 'dataset_group'])['scores_is_better']\n",
    "    .value_counts(normalize=True) * 100))\n",
    "\n",
    "print(\"\\nPercentage of 'is_better' overall:\")\n",
    "print((df['scores_is_better'].value_counts(normalize=True) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18b8c2f",
   "metadata": {},
   "source": [
    "### Feature Engineering (UNUSED)\n",
    "Create new features:\n",
    "- Calculate differences between consecutive steps for each layer\n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "038390b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "df = pd.read_csv('./model_stats.csv')\n",
    "\n",
    "# Define the layer steps you want to compare\n",
    "steps = ['step_10', 'step_25', 'step_50', 'step_100', 'step_200', 'step_300', 'step_400', 'step_500']\n",
    "layers = ['mlp.0', 'mlp.1', 'mlp.2']  # Adapt if you have more\n",
    "stats = ['weight_mean', 'weight_std', 'weight_var', 'weight_frobenius_norm', 'weight_spectral_norm', 'weight_alpha_hat']  # etc.\n",
    "\n",
    "# Find all relevant columns\n",
    "pattern = re.compile(r'weights_step_(\\d+)_mlp\\.(\\d+)\\.(\\w+)')\n",
    "relevant_cols = [col for col in df.columns if pattern.match(col)]\n",
    "\n",
    "# Compute diffs and store them in a dictionary first\n",
    "features = {}\n",
    "\n",
    "for stat in stats:\n",
    "    for layer in layers:\n",
    "        for i in range(1, len(steps)):\n",
    "            prev_step = steps[i-1]\n",
    "            curr_step = steps[i]\n",
    "            col_prev = f'weights_{prev_step}_{layer}.{stat}'\n",
    "            col_curr = f'weights_{curr_step}_{layer}.{stat}'\n",
    "            if col_prev in df.columns and col_curr in df.columns:\n",
    "                new_col = f'diff_{curr_step}_{prev_step}_{layer}_{stat}'\n",
    "                features[new_col] = df[col_curr] - df[col_prev]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31dc9c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Concatenate all new features at once\n",
    "# features_df = pd.DataFrame(features)\n",
    "# df = pd.concat([df, features_df], axis=1)\n",
    "\n",
    "# # Save or return the enhanced dataframe\n",
    "# df.to_csv('./new_model_stats.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af3c76f",
   "metadata": {},
   "source": [
    "## Stages Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "648546a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c20136c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LINE PLOTS for CLASSIFICATION METRICS\n",
    "df_classification = pd.read_csv('./output/classification/stagewise_summary.csv')\n",
    "\n",
    "# Print evolution of metrics for classification in separate plots\n",
    "base_metrics = ['acc_score', 'roc_auc_score', 'log_loss_score', 'f1_score']\n",
    "\n",
    "output_dir = os.path.join(\"output\", \"images\", \"classification\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for metric in base_metrics:\n",
    "    mean_col = f\"{metric}_mean\"\n",
    "    std_col = f\"{metric}_std\"\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.errorbar(df_classification['stage'], df_classification[mean_col],\n",
    "    # plt.errorbar(df_classification['stage'], df_classification[mean_col], yerr=df_classification[std_col],\n",
    "                 fmt='o-', capsize=5, label=metric)\n",
    "    plt.ylabel(metric)\n",
    "    plt.xlabel('Stage')\n",
    "    plt.title(f\"Evolution of '{metric}' (Classification)\")\n",
    "    plt.grid(True)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.legend()\n",
    "\n",
    "    plt.savefig(f\"{output_dir}/{metric}_evolution.png\")\n",
    "    # plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e15c3c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LINE PLOTS for REGRESSION METRICS\n",
    "df_regression = pd.read_csv('./output/regression/stagewise_summary.csv')\n",
    "\n",
    "# Print evolution of metrics for regression in separate plots\n",
    "base_metrics = ['mae_score', 'mse_score', 'r2_score', 'pearson', 'kendall', 'spearman']\n",
    "\n",
    "output_dir = os.path.join(\"output\", \"images\", \"regression\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for metric in base_metrics:\n",
    "    mean_col = f\"{metric}_mean\"\n",
    "    std_col = f\"{metric}_std\"\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.errorbar(df_regression['stage'], df_regression[mean_col],\n",
    "    # plt.errorbar(df_regression['stage'], df_regression[mean_col], yerr=df_regression[std_col],\n",
    "                 fmt='o-', capsize=5, label=metric)\n",
    "    plt.ylabel(metric)\n",
    "    plt.xlabel('Stage')\n",
    "    plt.title(f\"Evolution of '{metric}' (Regression)\")\n",
    "    plt.grid(True)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.legend()\n",
    "\n",
    "    plt.savefig(f\"{output_dir}/{metric}_evolution.png\")\n",
    "    # plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1912385",
   "metadata": {},
   "source": [
    "### Insights\n",
    "* **Stage 10 Performance Drop**: Sharp degradation (highest MAE/MSE, lowest R² and correlations) indicates unstable learning early on.\n",
    "* **Improvement Over Time**: MAE and MSE decline with more stages, showing better error control as training progresses.\n",
    "* **Negative R² Throughout**: R² remains < 0 at all stages—model fits worse than a naive mean predictor.\n",
    "* **Rank Correlation Improves**: Spearman and Kendall scores rise steadily to \\~0.76, indicating good relative ranking of predictions.\n",
    "* **Plateaus Detected**: Metrics stabilize after step\\_100 and step\\_400; performance gains flatten despite increased computation.\n",
    "* **High Early Variance**: Step\\_10 shows largest standard deviations—early predictions are inconsistent.\n",
    "* **Time vs Benefit**: Time increases \\~6× (3.45s → 22.04s) but error and correlation gains diminish after step\\_100.\n",
    "* **Actionable Insight**: Consider early stopping around step\\_100–200; review model design to address poor absolute accuracy (negative R²).\n",
    "* **Model Strength**: Performs well in ranking predictions, suitable for tasks where order matters more than exact values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d877eb32",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
